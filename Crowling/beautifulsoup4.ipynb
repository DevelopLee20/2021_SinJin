{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 94,
            "source": [
                "import requests\r\n",
                "from bs4 import BeautifulSoup\r\n",
                "\r\n",
                "def find_title(soup):\r\n",
                "    lst = []\r\n",
                "\r\n",
                "    title_list = soup.find(\"ul\",{\"class\": \"list_news\"})\r\n",
                "    title_list = title_list.find_all(\"li\")\r\n",
                "\r\n",
                "    for i in title_list:\r\n",
                "        try:\r\n",
                "            content = i.find(\"a\", {\"class\":\"news_tit\"}).text\r\n",
                "            lst.append(content)\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "\r\n",
                "    return lst\r\n",
                "\r\n",
                "def find_link(soup):\r\n",
                "    lst = []\r\n",
                "\r\n",
                "    link_list = soup.find(\"ul\", {\"class\": \"list_news\"})\r\n",
                "    link_list = link_list.find_all(\"li\")\r\n",
                "\r\n",
                "    for i  in link_list:\r\n",
                "        try:\r\n",
                "            content = i.find(\"a\", {\"class\":\"news_tit\"})\r\n",
                "            lst.append(content.get('href'))\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "    \r\n",
                "    return lst\r\n",
                "\r\n",
                "def find_sub_title(soup):\r\n",
                "    lst = []\r\n",
                "\r\n",
                "    title_list = soup.find(\"ul\",{\"class\": \"list_news\"})\r\n",
                "    title_list = title_list.find_all(\"span\",{\"class\":\"sub_wrap\"})\r\n",
                "\r\n",
                "    for i in title_list:\r\n",
                "        try:\r\n",
                "            content = i.find(\"a\",{\"class\":\"elss\"}).text\r\n",
                "            lst.append(content)\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "\r\n",
                "    return lst\r\n",
                "\r\n",
                "def find_sub_link(soup):\r\n",
                "    lst = []\r\n",
                "\r\n",
                "    link_list = soup.find(\"ul\",{\"class\": \"list_news\"})\r\n",
                "    link_list = link_list.find_all(\"span\",{\"class\":\"sub_wrap\"})\r\n",
                "\r\n",
                "    for i in link_list:\r\n",
                "        try:\r\n",
                "            content = i.find(\"a\",{\"class\":\"elss\"})\r\n",
                "            lst.append(content.get('href'))\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "\r\n",
                "    return lst\r\n",
                "\r\n",
                "search_text = '도쿄 올림픽'\r\n",
                "\r\n",
                "url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search_text}&start=1'\r\n",
                "content = requests.get(url)\r\n",
                "\r\n",
                "soup = BeautifulSoup(content.text, \"html\")\r\n",
                "\r\n",
                "title_list = find_title(soup)\r\n",
                "link_list = find_link(soup)\r\n",
                "sub_title_list = find_sub_title(soup)\r\n",
                "sub_link_list = find_sub_link(soup)\r\n",
                "\r\n",
                "len(title_list), len(sub_link_list)"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "(10, 12)"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 94
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 93,
            "source": [
                "import requests\r\n",
                "from bs4 import BeautifulSoup\r\n",
                "from openpyxl import Workbook\r\n",
                "import time\r\n",
                "\r\n",
                "search_text = input(\"Enter the search content : \")\r\n",
                "page = int(input(\"How many page : \"))\r\n",
                "\r\n",
                "number = -9\r\n",
                "\r\n",
                "title_list = []\r\n",
                "link_list = []\r\n",
                "\r\n",
                "print(f\"{search_text}의 검색결과를 찾는 중입니다 . . .\")\r\n",
                "print(f\"총 {page}페이지 검색\")\r\n",
                "\r\n",
                "for i in range(page):\r\n",
                "\r\n",
                "    print(f\"검색 페이지 수 : {i+1} / {page}\")\r\n",
                "\r\n",
                "    number += 10\r\n",
                "    url = f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search_text}&start={number}'\r\n",
                "    content = requests.get(url)\r\n",
                "\r\n",
                "    assert content.status_code == 200\r\n",
                "\r\n",
                "    soup = BeautifulSoup(content.text, \"html.parser\")\r\n",
                "\r\n",
                "    minseo = soup.find(\"ul\", {\"class\": \"list_news\"}).find_all(\"il\")\r\n",
                "\r\n",
                "    for i in minseo:\r\n",
                "        try:\r\n",
                "            title = i.find(\"a\", {\"class\":\"news_tit\"}).text\r\n",
                "            link = i.find(\"a\", {\"class\":\"news_tit\"}).get('href')\r\n",
                "            title_list.append(title)\r\n",
                "            link_list.append(link)\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "    \r\n",
                "    time.sleep(1)\r\n",
                "\r\n",
                "print(\"검색 완료!\")\r\n",
                "time.sleep(1)\r\n",
                "print(\"파일에 저장중 . . .\")\r\n",
                "\r\n",
                "wb = Workbook()\r\n",
                "ws = wb.active\r\n",
                "\r\n",
                "row = 2\r\n",
                "\r\n",
                "ws['A1'] = '뉴스명'\r\n",
                "ws['B1'] = '링크'\r\n",
                "\r\n",
                "for i, j in zip(title_list, link_list):\r\n",
                "    ws['A' + str(row)] = i\r\n",
                "    ws['A' + str(row)] = j\r\n",
                "    row += 1\r\n",
                "\r\n",
                "wb.save(f'{search_text}.xlsx')\r\n",
                "\r\n",
                "time.sleep(1)\r\n",
                "print(\"파일에 저장 완료!\")\r\n",
                "\r\n",
                "print(title_list, link_list)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "도쿄올림픽의 검색결과를 찾는 중입니다 . . .\n",
                        "총 4페이지 검색\n",
                        "검색 페이지 수 : 1 / 4\n",
                        "검색 페이지 수 : 2 / 4\n",
                        "검색 페이지 수 : 3 / 4\n",
                        "검색 페이지 수 : 4 / 4\n",
                        "검색 완료!\n",
                        "파일에 저장중 . . .\n",
                        "파일에 저장 완료!\n",
                        "[] []\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit"
        },
        "interpreter": {
            "hash": "d54bb5494ef5283f3bb9da334a12126f2e168d7022622466af04c6570fd1119d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}